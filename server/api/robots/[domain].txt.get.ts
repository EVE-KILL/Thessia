/**
 * Dynamic robots.txt generation for custom domains
 * Generates domain-specific robots.txt with proper sitemap references
 */
export default defineEventHandler(async (event) => {
    const domain = getRouterParam(event, "domain");

    if (!domain) {
        throw createError({
            statusCode: 400,
            statusMessage: "Domain parameter required",
        });
    }

    try {
        // Find the custom domain configuration
        const domainConfig = await CustomDomains.findOne({
            domain: domain.toLowerCase(),
            active: true,
            verified: true,
        });

        if (!domainConfig) {
            throw createError({
                statusCode: 404,
                statusMessage: "Domain not found or not active",
            });
        }

        const baseUrl = `https://${domain}`;

        // Generate robots.txt content
        let robotsTxt = `# Robots.txt for ${domain}\n`;
        robotsTxt += `# Generated by EVE-KILL\n\n`;

        // Allow all bots by default
        robotsTxt += `User-agent: *\n`;
        robotsTxt += `Allow: /\n\n`;

        // Disallow admin and API paths (if they exist on custom domains)
        robotsTxt += `# Disallow admin and API paths\n`;
        robotsTxt += `Disallow: /admin/\n`;
        robotsTxt += `Disallow: /api/\n`;
        robotsTxt += `Disallow: /_nuxt/\n\n`;

        // Set crawl delay for politeness
        robotsTxt += `# Crawl delay\n`;
        robotsTxt += `Crawl-delay: 1\n\n`;

        // Add sitemap reference
        robotsTxt += `# Sitemap\n`;
        robotsTxt += `Sitemap: ${baseUrl}/sitemap.xml\n`;

        // Set appropriate headers
        setResponseHeaders(event, {
            "Content-Type": "text/plain; charset=utf-8",
            "Cache-Control": "public, max-age=86400", // Cache for 24 hours
        });

        return robotsTxt;
    } catch (error) {
        console.error("Error generating robots.txt for domain:", domain, error);
        throw createError({
            statusCode: 500,
            statusMessage: "Failed to generate robots.txt",
        });
    }
});
